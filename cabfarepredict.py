# -*- coding: utf-8 -*-
"""CabFarePredict.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fsZJZaHIJ5AXRGWTbPu_lcz6c7NnNpiM

**PROBLEM STATEMENT**

You are a cab rental start-up company. You have successfully run the pilot project and now want to launch your cab service across the country. You have collected the historical data from your pilot project and now have a requirement to apply analytics for fare prediction. You need to design a system that predicts the fare amount for a cab ride in the city.
"""

# Commented out IPython magic to ensure Python compatibility.
import io
import os             # getting access to input files
import pandas as pd   # importing pandas for performing EDA
import numpy as np    # importing numpy for linear algebra functions
import matplotlib.pyplot as plt # importing for data visualization
import seaborn as sns # importing for data visualization
import scipy.stats as stats
from fancyimpute import KNN
from geopy.distance import geodesic
from geopy.distance import great_circle
from scipy.stats import chi2_contingency
import statsmodels.api as sm
from statsmodels.formula.api import ols
from patsy import dmatrices
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn import metrics

from sklearn.linear_model import LinearRegression
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score

from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import LinearRegression #ML algorithm
from sklearn.model_selection import train_test_split #splitting dataset
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from pprint import pprint
from sklearn.model_selection import GridSearchCV 
from xgboost import XGBRegressor
import xgboost as xgb
from sklearn.externals import joblib

# %matplotlib inline

from google.colab import files
uploaded = files.upload()

df = pd.read_csv(io.BytesIO(uploaded['train_cab.csv']))

from google.colab import files
uploaded = files.upload()

df = pd.read_csv(io.BytesIO(uploaded['test.csv']))

"""**UNDERSTANDING THE DATA**"""

train  = pd.read_csv("train_cab.csv",na_values={"pickup_datetime":"43"})
test   = pd.read_csv("test.csv")

train.head() # checking the first 5 rows of training dataset

# Importing data
train = pd.read_csv('train_cab.csv',dtype={'fare_amount':np.float64},na_values={'fare_amount':'430-'})
test = pd.read_csv('test.csv')
data=[train,test]
for i in data:
    i['pickup_datetime']  = pd.to_datetime(i['pickup_datetime'],errors='coerce')
train.head(5)

train.info()

test.head()

test.info()

print("the shape of the training data is: ", train.shape)
print("the shape of the test data is :", test.shape)

train.dtypes # checking the datatypes in training dataset

test.dtypes # checking the datatypes in test dataset

train.describe()

test.describe()

"""**EDA**"""

# We will convert passenger count to the categorical variable coz passenger_count in not a continuous variables
# passenger count can not take continuous variables and also passenger count are limited in no if it is cab
cat_var = ['passenger_count']
num_var = ['fare_amount', 'pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']

"""**DATA VISUALIZATION**"""

# setting up the sns for plots
sns.set(style = 'darkgrid', palette = 'Set1')

# some histogram plots from seaborn library
plt.figure(figsize=(20,20))
plt.subplot(321)
hist = sns.distplot(train['pickup_longitude'],bins=50)
plt.subplot(322)
hist = sns.distplot(train['pickup_latitude'],bins=50)
plt.subplot(323)
hist = sns.distplot(train['dropoff_longitude'],bins=50)
plt.subplot(324)
hist = sns.distplot(train['dropoff_latitude'],bins=50)

# Joint plots for bivariate analysis
# here scatter plots have regression line between 2 variables along with seperate barplots for both the variables
# also it annoted with pearson correlation coefficients and p value
# target variable  fare_amount vs each numeric variable
plot = sns.jointplot(x='fare_amount',y='pickup_longitude',data=train,kind = 'reg')
plot.annotate(stats.pearsonr)
plt.show()

plot = sns.jointplot(x='fare_amount',y='pickup_latitude',data=train,kind = 'reg')
plot.annotate(stats.pearsonr)
plt.show()

plot = sns.jointplot(x='fare_amount',y='dropoff_longitude',data=train,kind = 'reg')
plot.annotate(stats.pearsonr)
plt.show()

plot = sns.jointplot(x='fare_amount',y='dropoff_latitude',data=train,kind = 'reg')
plot.annotate(stats.pearsonr)
plt.show()

"""Some violin plots to see the distribution"""

plt.figure(figsize = (20,20))
plt.subplot(321)
plot = sns.violinplot(y = 'fare_amount', data = train)
plt.subplot(322)
plot = sns.violinplot(y = 'pickup_latitude', data = train)
plt.subplot(323)
plot = sns.violinplot(y = 'pickup_longitude', data = train)
plt.subplot(324)
plot = sns.violinplot(y = 'dropoff_longitude', data = train)
plt.subplot(325)
plot = sns.violinplot(y = 'dropoff_latitude', data = train)

"""Pairplot all the numeric variables"""

plot = sns.pairplot(data = train[num_var], kind = 'scatter', dropna = True)
plot.fig.suptitle('Pairwise distribution of numeric variables')
plt.show()

"""**Removing values which are not in the desired range(outlier) depending upon basic understanding of dataset.**

1. Fare Amount have negative values which does not make sense coz fare can not be negative or 0 . so we will remove those values
"""

sum(train['fare_amount']<1)

train[train['fare_amount']<1]

train = train.drop(train[train['fare_amount']<1].index, axis=0)

"""2. Passenger_count variable"""

for i in range (4,11):
    print('passenger count above'+str(i)+':={}'.format(sum(train['passenger_count']>i)))
    
# So 20 observations of passenger_count are consistently above from 6,7,8,9,10  passenger_count, let's cheq them

train[train['passenger_count']>6]

# also we need to see if there is any passenger_count less that 1
train[train['passenger_count']<1]

len(train[train['passenger_count']<1])

test['passenger_count'].unique()

"""*   Passenger count contains values which are equal  to 0
*   And test data does not contains passenger_count = 0, So if we will feature engineer passenger_count of train dataset, then it will create a dummy variable of passenger_count = 0 which will be an extra feature compared to a test data set.
*   So we will remove those 0 values.
*   Also we will remove 20 variables which are above 6 coz a cob does not contains that much passengers.
"""

train = train.drop(train[train['passenger_count']>6].index, axis = 0)
train = train.drop(train[train['passenger_count']<1].index, axis = 0)

# checking
sum(train['passenger_count']>6)

"""3. Latitude ranges from -90 to +90 and Longitutes ranges from -180 to +180. Removing those values which does not satisfy these range."""

print('Pickup_longitude above 180 = {}'.format(sum(train['pickup_longitude']>180)))
print('Pickup_longitude below -180 = {}'.format(sum(train['pickup_longitude']< -180)))
print('Pickup_latitude above 90 = {}'.format(sum(train['pickup_latitude']>90)))
print('Pickup_latitude below -90 = {}'.format(sum(train['pickup_latitude']< -90)))
print('dropoff_longitude above 180 = {}'.format(sum(train['dropoff_longitude']>180)))
print('dropoff_longitude below -180 = {}'.format(sum(train['dropoff_longitude']< -180)))
print('dropoff_latitude above 90 = {}'.format(sum(train['dropoff_latitude']>180)))
print('dropoff_latitude below -90 = {}'.format(sum(train['dropoff_latitude']< -180)))

# thers is only 1 outlier which is in variable pickup_latitude, So we will remove it with nan
# Also we will see if there is any value equal to 0

for i in ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']:
    print(i, 'equal to 0={}'.format(sum(train[i]==0)))
    
# There are values which are equal to 0 we will remove them

train = train.drop(train[train['pickup_latitude']>90].index, axis = 0)
for i in ['pickup_latitude', 'pickup_longitude', 'dropoff_latitude', 'dropoff_longitude']:
    train = train.drop(train[train[i]==0].index, axis = 0)

train.shape

df = train.copy()

"""**MISSING VALUE ANALYSIS**"""

# Create a datafreame with missing percentage
missing_val = pd.DataFrame(train.isnull().sum())
# Reset index
missing_val = missing_val.reset_index()
missing_val

"""*   As we can see that there are some missing values in data
*   Also pickup_datetime has 1 missing value
*   We will impute missing values for fare_amount, passenger_count values but not the pickup_datetime
*   And we will drop that 1 row which has missing values in pickup_datetime.
"""

# Rename variables
missing_val = missing_val.rename(columns = {'index': 'Variables', 0 : 'Missing_Percentage'})
missing_val
# Calculate percentage
missing_val['Missing_Percentage'] = (missing_val['Missing_Percentage']/len(train))*100
# Descending order
missing_val = missing_val.sort_values('Missing_Percentage', ascending = False).reset_index(drop = True)
missing_val

"""1. For Passenger_count
    
    Actual Value = 1
    
    Mode = 1
   
    KNN  = 2
"""

# Choosing a random value to rplace it with na
train['passenger_count'].loc[1000]

# Replacing 1.0 with na
train['passenger_count'].loc[1000] = np.nan
train['passenger_count'].loc[1000]

# impute with mode
train['passenger_count'].fillna(train['passenger_count'].mode()[0]).loc[1000]

"""We cant use mode method coz data will be more biased towards passenger_count = 1

2. For fare amount 
      
      Actual value = 7.0
      
      Mean = 15.1
      
      Media = 8.5
      
      KNN = 7.3
"""

# Choosing the randon values to replace it with NA
a = train['fare_amount'].loc[1000]
print('fare amount at loc-1000: {}'.format(a))

# Replace it with na
train['fare_amount'].loc[1000] = np.nan
train['fare_amount'].loc[1000]

# Impute with mean
print('value if imputed with mean= {}'.format(train['fare_amount'].fillna(train['fare_amount'].mean()).loc[1000]))

# Impute with median
print('value if imputed with median= {}'.format(train['fare_amount'].fillna(train['fare_amount'].median()).loc[1000]))

columns=['fare_amount', 'pickup_longitude', 'pickup_latitude','dropoff_longitude', 'dropoff_latitude', 'passenger_count']

"""We will seperate pickup datetime in seperate dataframe and then merge them with train feature engineering step"""

pickup_datetime = pd.DataFrame(train['pickup_datetime'])

# Now imputing missing values using KNN
# Using 19 nearest rows which have a feature to fill in each rows missing feature
train = pd.DataFrame(KNN(k=19).fit_transform(train.drop('pickup_datetime',axis = 1)), columns = columns, index = train.index)

train.std()

train.loc[1000]

train['passenger_count'].head()

train['passenger_count'] = train['passenger_count'].astype('int')

train['passenger_count'].head()

train.std()

train['passenger_count'].unique()

train['passenger_count']=train['passenger_count'].round().astype('object').astype('category')

train['passenger_count'].unique()

train.loc[1000]

"""Now missing values in pickup_datetime"""

pickup_datetime.head()

# Create dataframe with missing percentage
missing_val = pd.DataFrame(pickup_datetime.isnull().sum())
# Reset index
missing_val = missing_val.reset_index()
missing_val

pickup_datetime.shape

train.shape

"""*   We will drop 1 row which has missing value for pickup_datetime variable after feature engineering step because if we drop now, pickup_datetime dataframe will have 16040 rows and our train has 1641 rows, then if we merge these 2 dataframes then pickup_datetime variable will gain 1 missing value.
*   And if we merge and then drop now then we would require to split again before outlier analysis and then merge again in feature engineering step.
*   So, instead of doing the work 2 times we will drop 1 time i.e. after feature engineering process.
"""

df1 = train.copy()

train['passenger_count'].describe()

train.describe()

"""**OUTLIER ANALYSIS USING BOXPLOT**

We will do outlier analysis on fare amount just for now and we will do outlier analysis after feature engineering latitudes and longitudes.
*   Univariate Boxplots: Boxplots for numerical variable using target variable.
"""

plt.figure(figsize = (20,5))
plt.xlim(0, 100)
sns.boxplot(x = train['fare_amount'], data = train)
plt.title('Boxplot for fare_amount')
plt.show

"""Bivariate Boxplots = boxplot for numeric variable vs the categorical variab;e"""

plt.figure(figsize = (20,10))
plt.xlim(0,100)
plot = sns.boxplot(x = train['fare_amount'], y = train['passenger_count'], data = train)
plt.title('Boxplot for fare_amount w.r.t passenger_count')
plt.show()

"""**OUTLIER TREATMENT**

*   As we have seen from the above data set that there are outliers in the train dataset.
*   Reconsider pickup_longitude etc.
"""

def outlier_treatment(col):
#... calculating outlier indices and replacing them with na...
# Extract quartiles
  q75, q25 = np.percentile(train[col], [75,25])
  print(q75, q25)
# Calculate iqr
  iqr = q75-q25
# Calculate inner and outer fence
  minimum = q25 - (iqr*1.5)
  maximum = q75 + (iqr*1.5)
  print(minimum, maximum)
# Replace with NA
  train.loc[train[col] < minimum, col] = np.nan
  train.loc[train[col] > maximum, col] = np.nan

outlier_treatment('fare_amount')

# we have replaced outliers in fare amount with na so down with the help of knn we will impute them
pd.DataFrame(train.isnull().sum())

#Imputing with missing values using KNN
train = pd.DataFrame(KNN(k = 3).fit_transform(train), columns = train.columns, index=train.index)

train.std()

train['passenger_count'].describe()

train['passenger_count']=train['passenger_count'].astype('int').round().astype('object').astype('category')

train.describe()

train.head()

df2 = train.copy()

train.shape

"""**FEATURE ENGINEERING**

*   we will derive new features from pickup_datetime variable
*   new features will be year,month,day_of_week,hour
"""

# we will Join 2 Dataframes pickup_datetime and train
train = pd.merge(pickup_datetime,train,right_index=True,left_index=True)
train.head()

train.shape

train=train.reset_index(drop=True)

pd.DataFrame(train.isna().sum())

train=train.dropna()

data = [train,test]
for i in data:
    i["year"] = i["pickup_datetime"].apply(lambda row: row.year)
    i["month"] = i["pickup_datetime"].apply(lambda row: row.month)
    i["day_of_week"] = i["pickup_datetime"].apply(lambda row: row.dayofweek)
    i["hour"] = i["pickup_datetime"].apply(lambda row: row.hour)

plt.figure(figsize=(20,10))
sns.countplot(train['year'])

plt.figure(figsize=(20,10))
sns.countplot(train['month'])

plt.figure(figsize=(20,10))
sns.countplot(train['day_of_week'])

plt.figure(figsize=(20,10))
sns.countplot(train['hour'])

def f(x):
    ''' for sessions in a day using hour column '''
    if (x >=5) and (x <= 11):
        return 'morning'
    elif (x >=12) and (x <=16 ):
        return 'afternoon'
    elif (x >= 17) and (x <= 20):
        return'evening'
    elif (x >=21) and (x <= 23) :
        return 'night_PM'
    elif (x >=0) and (x <=4):
        return'night_AM'

def g(x):
    ''' for seasons in a year using month column'''
    if (x >=3) and (x <= 5):
        return 'spring'
    elif (x >=6) and (x <=8 ):
        return 'summer'
    elif (x >= 9) and (x <= 11):
        return'fall'
    elif (x >=12)|(x <= 2) :
        return 'winter'

def h(x):
    ''' for week:weekday/weekend in a day_of_week column '''
    if (x >=0) and (x <= 4):
        return 'weekday'
    elif (x >=5) and (x <=6 ):
        return 'weekend'

train['session'] = train['hour'].apply(f)
test['session'] = test['hour'].apply(f)

train['seasons'] = train['month'].apply(g)
test['seasons'] = test['month'].apply(g)

train['week'] = train['day_of_week'].apply(h)
test['week'] = test['day_of_week'].apply(h)

train.shape

test.shape

"""**2. Feature engineering fot passenger_count variable**

*   Because models in scikit learn require numerical input,if dataset contains categorical variables then we have to encode them.

*   
We will use one hot encoding technique for passenger_count variable.
"""

train['passenger_count'].describe()

#Creating dummies for each variable in passenger_count and merging dummies dataframe to both train and test dataframe
temp = pd.get_dummies(train['passenger_count'], prefix = 'passenger_count')
train = train.join(temp)
temp = pd.get_dummies(test['passenger_count'], prefix = 'passenger_count')
test = test.join(temp)
temp = pd.get_dummies(train['seasons'], prefix = 'season')
train = train.join(temp)
temp = pd.get_dummies(test['seasons'], prefix = 'season')
test = test.join(temp)
temp = pd.get_dummies(train['week'], prefix = 'week')
train = train.join(temp)
temp = pd.get_dummies(test['week'], prefix = 'week')
test = test.join(temp)
temp = pd.get_dummies(train['session'], prefix = 'session')
train = train.join(temp)
temp = pd.get_dummies(test['session'], prefix = 'session')
test = test.join(temp)
temp = pd.get_dummies(train['year'], prefix = 'year')
train = train.join(temp)
temp = pd.get_dummies(test['year'], prefix = 'year')
test = test.join(temp)

train.head()

test.head()

train.columns

train=train.drop(['passenger_count_1','season_fall','week_weekday','session_afternoon','year_2009'],axis=1)
test=test.drop(['passenger_count_1','season_fall','week_weekday','session_afternoon','year_2009'],axis=1)

"""**3. Feature engineering for latitude and longitude variable**

*   As we have latitude and longitude data for pickup and dropoff, we will find the distance the cab travelled from pickup and dropoff location.
"""

# Calculate distance the cab travelled from pickup and dropoff location using great_circle from geopy library
data = [train, test]
for i in data:
    i['great_circle']=i.apply(lambda x: great_circle((x['pickup_latitude'],x['pickup_longitude']), (x['dropoff_latitude'],   x['dropoff_longitude'])).miles, axis=1)
    i['geodesic']=i.apply(lambda x: geodesic((x['pickup_latitude'],x['pickup_longitude']), (x['dropoff_latitude'],   x['dropoff_longitude'])).miles, axis=1)

train.head()

test.head()

"""As Vincenty is more accurate than haversine. Also vincenty is prefered for short distances.Therefore we will drop great_circle. we will drop them together with other variables which were used to feature engineer."""

pd.DataFrame(train.isna().sum())

pd.DataFrame(test.isna().sum())

"""We will remove the variables which were used to feature engineer new variables."""

train=train.drop(['pickup_datetime','pickup_longitude', 'pickup_latitude',
       'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'year',
       'month', 'day_of_week', 'hour', 'session', 'seasons', 'week','great_circle'],axis=1)
test=test.drop(['pickup_datetime','pickup_longitude', 'pickup_latitude',
       'dropoff_longitude', 'dropoff_latitude', 'passenger_count', 'year',
       'month', 'day_of_week', 'hour', 'session', 'seasons', 'week','great_circle'],axis=1)

train.shape,test.shape

train.columns

test.columns

train.head()

test.head()

plt.figure(figsize=(20,5)) 
plt.xlim(0,100)
sns.boxplot(x=train['geodesic'],data=train,orient='h')
plt.title('Boxplot of geodesic ')
plt.show()

outlier_treatment('geodesic')

pd.DataFrame(train.isnull().sum())

#Imputing with missing values using KNN
train = pd.DataFrame(KNN(k = 3).fit_transform(train), columns = train.columns, index=train.index)

"""**FEATURE SELECTION**

*   Statistically correlated: features move together directionally.
*   Linear models assume feature independence.
And if features are correlated that could introduce bias into our models.
"""

cat_var=['passenger_count_2',
       'passenger_count_3', 'passenger_count_4', 'passenger_count_5',
       'passenger_count_6', 'season_spring', 'season_summer',
       'season_winter', 'week_weekend',
       'session_evening', 'session_morning', 'session_night_AM',
       'session_night_PM', 'year_2010', 'year_2011',
       'year_2012', 'year_2013', 'year_2014', 'year_2015']
num_var=['fare_amount','geodesic']
train[cat_var]=train[cat_var].apply(lambda x: x.astype('category') )
test[cat_var]=test[cat_var].apply(lambda x: x.astype('category') )

"""*   We will plot a Heatmap of correlation whereas, correlation measures how strongly 2 quantities are related to each other."""

# heatmap using correlation matrix
plt.figure(figsize=(15,15))
_ = sns.heatmap(train[num_var].corr(), square=True, cmap='RdYlGn',linewidths=0.5,linecolor='w',annot=True)
plt.title('Correlation matrix ')
plt.show()

"""As we can see from above correlation plot fare_amount and geodesic is correlated to each other.

*   Jointplots for Bivariate Analysis
*   Here Scatter plot has regression line between 2 variables along with separate Bar plots of both variables.
*   Also its annotated with pearson correlation coefficient and p value.
"""

plot = sns.jointplot(x='fare_amount',y='geodesic',data=train,kind = 'reg')
plot.annotate(stats.pearsonr)
plt.show()

"""**CHI-SQUARE TEST OF INDEPENDENCE FOR CATEGORICAL VARIABLES/FEATURES**

*   Hypothesis testing :
         Null Hypothesis: 2 variables are independent.
         
         Alternate Hypothesis: 2 variables are not independent.
         
*   If p-value is less than 0.05 then we reject the null hypothesis saying that 2 variables are dependent.
*   And if p-value is greater than 0.05 then we accept the null hypothesis saying that 2 variables are independent.
*   There should be no dependencies between Independent variables.
*   So we will remove that variable whose p-value with other variable is low than 0.05.
*   And we will keep that variable whose p-value with other variable is high than 0.05
"""

#loop for chi square values
for i in cat_var:
    for j in cat_var:
        if(i != j):
            chi2, p, dof, ex = chi2_contingency(pd.crosstab(train[i], train[j]))
            if(p < 0.05):
                print(i,"and",j,"are dependent on each other with",p,'----Remove')
            else:
                print(i,"and",j,"are independent on each other with",p,'----Keep')

"""**ANALYSIS OF VARIANCE(ANOVA) TEST**

*   It is carried out to compare between each groups in a categorical variable.
*   ANOVA only lets us know the means for different groups are same or not. It doesn’t help us identify which mean is different.
*   Hypothesis testing :
       Null Hypothesis: mean of all categories in a variable are same.
       Alternate Hypothesis: mean of at least one category in a variable is different.
*   If p-value is less than 0.05 then we reject the null hypothesis.
And if p-value is greater than 0.05 then we accept the null hypothesis.
"""

train.columns

model = ols('fare_amount ~ C(passenger_count_2)+C(passenger_count_3)+C(passenger_count_4)+C(passenger_count_5)+C(passenger_count_6)+C(season_spring)+C(season_summer)+C(season_winter)+C(week_weekend)+C(session_night_AM)+C(session_night_PM)+C(session_evening)+C(session_morning)+C(year_2010)+C(year_2011)+C(year_2012)+C(year_2013)+C(year_2014)+C(year_2015)',data=train).fit()
                
aov_table = sm.stats.anova_lm(model)
aov_table

"""Every variable has p-value less than 0.05 therefore we reject the nul hypothesis

**MULTICOLLINEARITY TEST**

*   VIF should alwaz greater than or equal to 1.
*   If VIF is 1 --- means not correlated to any of the variables.
*   If VIF is between 1-5 means moderately correlated
*   If VIF is above 5 means highly correlated.
*   If there are multiple variables of VIF greater than 5,only remove variable with highest VIF.
"""

train.columns

outcome, predictors = dmatrices('fare_amount ~ geodesic+passenger_count_2+passenger_count_3+passenger_count_4+passenger_count_5+passenger_count_6+season_spring+season_summer+season_winter+week_weekend+session_night_AM+session_night_PM+session_evening+session_morning+year_2010+year_2011+year_2012+year_2013+year_2014+year_2015',train, return_type='dataframe')

# Calculating VIF for each individual predictors
vif = pd.DataFrame()
vif["VIF"] = [variance_inflation_factor(predictors.values, i) for i in range(predictors.shape[1])]
vif["features"] = predictors.columns
vif

"""So, we have no or very low multicollinearity

**FEATURE SCALING CHEQ WITH OR WITHOUT NORMALIZATION OF STANDARSCALAR**
"""

train[num_var].var()

sns.distplot(train['geodesic'], bins = 50)

plt.figure()
stats.probplot(train['geodesic'], dist = 'norm', fit = True, plot = plt)

# Normalization
train['geodesic'] = (train['geodesic'] - min(train['geodesic'])) / (max(train['geodesic']) - min(train['geodesic']))
test['geodesic'] = (test['geodesic'] - min(test['geodesic'])) / (max(test['geodesic']) - min(test['geodesic']))

train['geodesic'].var()

sns.distplot(train['geodesic'], bins = 50)

plt.figure()
stats.probplot(train['geodesic'], dist = 'norm', fit = True, plot = plt)

train.columns

df4=train.copy()
f4=test.copy()

train = train.drop(['passenger_count_2'], axis = 1)
test = test.drop(['passenger_count_2'], axis = 1)

train.columns

"""**Splitting train into train and validation subsets**

*   X_train, Y_train are train subset.  
*   X_test,  Y_test are validation subset.
"""

X = train.drop('fare_amount',axis=1).values
y = train['fare_amount'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state=42)
print(train.shape, X_train.shape, X_test.shape,y_train.shape,y_test.shape)

def rmsle(y,y_):
    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))
    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))
    calc = (log1 - log2) ** 2
    return np.sqrt(np.mean(calc))
def scores(y, y_):
    print('r square  ', metrics.r2_score(y, y_))
    print('Adjusted r square:{}'.format(1 - (1-metrics.r2_score(y, y_))*(len(y)-1)/(len(y)-X_train.shape[1]-1)))
    print('MAPE:{}'.format(np.mean(np.abs((y - y_) / y))*100))
    print('MSE:', metrics.mean_squared_error(y, y_))
    print('RMSE:', np.sqrt(metrics.mean_squared_error(y, y_))) 
def test_scores(model):
    print('<<<------------------- Training Data Score --------------------->')
    print()
    #Predicting result on Training data
    y_pred = model.predict(X_train)
    scores(y_train,y_pred)
    print('RMSLE:',rmsle(y_train,y_pred))
    print()
    print('<<<------------------- Test Data Score --------------------->')
    print()
    # Evaluating on Test Set
    y_pred = model.predict(X_test)
    scores(y_test,y_pred)
    print('RMSLE:',rmsle(y_test,y_pred))

"""**MULTIPLE LINEAR REGRESSION**

*   there are many hyperparameter optimization/tuning algorithms now, there are two simple strategies: 1. grid search and 2. Random Search.
*   A model parameter is a configuration variable that is internal to the model and whose value can be estimated from the given data. but A model hyperparameter is a configuration that is external to the model and whose value cannot be estimated from data.
"""

# Setup the parameters and distributions to sample from: param_dist
param_dist = {'copy_X':[True, False],
          'fit_intercept':[True,False]}
# Instantiate a Decision reg classifier: reg
reg = LinearRegression()

# Instantiate the gridSearchCV object: reg_cv
reg_cv = GridSearchCV(reg, param_dist, cv=5,scoring='r2')

# Fit it to the data
reg_cv.fit(X, y)

# Print the tuned Decision Reg parameters and score
print("Tuned Decision reg Parameters: {}".format(reg_cv.best_params_))
print("Best score is {}".format(reg_cv.best_score_))

# Create the regressor: reg_all
reg_all = LinearRegression(copy_X= True, fit_intercept=True)

# Fit the regressor to the training data
reg_all.fit(X_train,y_train)

# Predict on the test data: y_pred
y_pred = reg_all.predict(X_test)

# Compute and print R^2 and RMSE
print("R^2: {}".format(reg_all.score(X_test, y_test)))
rmse = np.sqrt(mean_squared_error(y_test,y_pred))
print("Root Mean Squared Error: {}".format(rmse))
test_scores(reg_all)

# Compute and print the coefficients
reg_coef = reg_all.coef_
print(reg_coef)


# Plot the coefficients
plt.figure(figsize = (15,15))
plt.plot(range(len(test.columns)), reg_coef)
plt.xticks(range(len(test.columns)), test.columns.values, rotation=60)
plt.margins(0.02)
plt.show()



from sklearn.model_selection import cross_val_score
# Create a linear regression object : reg
reg = LinearRegression()

# Compute 5-fold cross validation scores: cv_score
cv_scores = cross_val_score(reg, X, y, cv = 5, scoring = 'neg_mean_squared_error')

# Print the 5-fold cross validation scores
print(cv_scores)

print("Average 5-fold cv Score : {}".format(np.mean(cv_scores)))

"""**Decision Tree Regression**"""

train.info()

"""need of instantiating grid search cv on decision tree?"""

# Setup the parameters and distributions to sample from: param_dist
param_dist = {'max_depth': range(2,16,2),
              'min_samples_split': range(2,16,2)}

# Instantiate a Decision Tree classifier: tree
tree = DecisionTreeRegressor()

# Instantiate the gridSearchCV object: tree_cv
tree_cv = GridSearchCV(tree, param_dist, cv=5)

# Fit it to the data
tree_cv.fit(X, y)

# Print the tuned parameters and score
print("Tuned Decision Tree Parameters: {}".format(tree_cv.best_params_))
print("Best score is {}".format(tree_cv.best_score_))

# Instantiate the tree regressor: tree
tree = DecisionTreeRegressor(max_depth = 6, min_samples_split = 2)

# Fit the regressor to the data
tree.fit(X_train, y_train)

# Compute and print the coefficients
tree_features = tree.feature_importances_
print(tree_features)

# Sort test importances in descending order
indices = np.argsort(tree_features)[::1]

# Rearrange test names so that they match sorted test importances
names = [test.columns[i] for i in indices]

# Creating the plot
fig = plt.figure(figsize = (20,10))
plt.title("test_importance")

# Add horizontal bars 
plt.barh(range(pd.DataFrame(X_train).shape[1]), tree_features[indices], align = 'center')
plt.yticks(range(pd.DataFrame(X_train).shape[1]), names)
plt.show()

# Make predictions and calculation errors
test_scores(tree)

"""**RANDOM FOREST REGRESSION**"""

# Create the random grid
random_grid = {'n_estimators': range(100, 500, 100),
              'max_depth': range(5, 20, 1),
              'min_samples_leaf': range(2, 5, 1),
               'max_features': ['auto', 'sqrt', 'log2'],
               'bootstrap': [True, False],
               'min_samples_split': range(2, 5, 1)}

# Instantiate the random forest classifier: Forest
Forest = RandomForestRegressor()

# Instantiate the grid search cv object: Forest_cv
Forest_cv = RandomizedSearchCV(Forest, random_grid, cv = 5)

# Fit it to the data
Forest_cv.fit(X, y)

# Print the tuned parameters and score
print("Tuned Random Forest Parameters: {}".format(Forest_cv.best_params_))
print("Best score is {}".format(Forest_cv.best_score_))

# Instantiate a forest regressor : Forest
Forest = RandomForestRegressor(n_estimators = 100, min_samples_split = 3, min_samples_leaf = 2, max_features = 'auto', max_depth = 10, bootstrap = True)

# Fit the regressor to the data
Forest.fit(X_train, y_train)

# Compute and print the coefficients
Forest_features = Forest.feature_importances_
print(Forest_features)

# Sort the feature importances in descending order
indices = np.argsort(Forest_features)[::1]

# Rearrange feature names so they match the sorted feature importances
names = [test.columns[i] for i in indices]


# Creating plot
fig = plt.figure(figsize=(20,10))
plt.title("Feature Importance")

# Add horizontal bars
plt.barh(range(pd.DataFrame(X_train).shape[1]),Forest_features[indices],align = 'center')
plt.yticks(range(pd.DataFrame(X_train).shape[1]), names)
plt.show()

# Make predictions
test_scores(Forest)

from sklearn.model_selection import cross_val_score
# Instantiate a forest regressor : Forest
Forest = RandomForestRegressor(n_estimators = 100, min_samples_split = 3, min_samples_leaf = 2, max_features = 'auto', max_depth = 10, bootstrap = True)

# Compute 5-fold cross-validation scores: cv_scores
cv_scores = cross_val_score(Forest,X,y,cv=5,scoring='neg_mean_squared_error')
print(cv_scores)

print("Average 5-Fold CV Score: {}".format(np.mean(cv_scores)))

"""**IMPROVING ACCURACY USING XGBOOST**

Improve accuracy 1. Algorithm tuning 2. Ensembles
"""

data_dmatrix = xgb.DMatrix(data=X,label=y)
dtrain = xgb.DMatrix(X_train, label=y_train)
dtest = xgb.DMatrix(X_test)

dtrain, dtest, data_dmatrix

params = {"objective":"reg:linear",'colsample_bytree': 0.3,'learning_rate': 0.1,
                'max_depth': 5, 'alpha': 10}

cv_results = xgb.cv(dtrain=data_dmatrix, params=params, nfold=5,
                    num_boost_round=50,early_stopping_rounds=10,metrics="rmse", as_pandas=True, seed=123)
cv_results.head()

# The final boosting round metric
print((cv_results["test-rmse-mean"]).tail(1))

Xgb = XGBRegressor()
Xgb.fit(X_train,y_train)

test_scores(Xgb)

# Create the random grid
para = {'n_estimators': range(100,500,100),
               'max_depth': range(3,10,1),
        'reg_alpha':np.logspace(-4, 0, 50),
        'subsample': np.arange(0.1,1,0.2),
        'colsample_bytree': np.arange(0.1,1,0.2),
        'colsample_bylevel': np.arange(0.1,1,0.2),
        'colsample_bynode': np.arange(0.1,1,0.2),
       'learning_rate': np.arange(.05, 1, .05)}
# Instantiate a Decision Forest classifier: Forest
Xgb = XGBRegressor()

# Instantiate the gridSearchCV object: Forest_cv
xgb_cv = RandomizedSearchCV(Xgb, para, cv=5)

# Fit it to the data
xgb_cv.fit(X, y)

# Print the tuned parameters and score
print("Tuned Xgboost Parameters: {}".format(xgb_cv.best_params_))
print("Best score is {}".format(xgb_cv.best_score_))

# Instantiate a xgb regressor: xgb
Xgb = XGBRegressor(subsample= 0.1, reg_alpha= 0.08685113737513521, n_estimators= 200, max_depth= 3, learning_rate=0.05, colsample_bytree= 0.7000000000000001, colsample_bynode=0.7000000000000001, colsample_bylevel=0.9000000000000001)

# Fit the regressor to the data
Xgb.fit(X_train,y_train)

# Compute and print the coefficients
xgb_features = Xgb.feature_importances_
print(xgb_features)

# Sort feature importances in descending order
indices = np.argsort(xgb_features)[::1]

# Rearrange feature names so they match the sorted feature importances
names = [test.columns[i] for i in indices]

# Creating plot
fig = plt.figure(figsize=(20,10))
plt.title("Feature Importance")

# Add horizontal bars
plt.barh(range(pd.DataFrame(X_train).shape[1]),xgb_features[indices],align = 'center')
plt.yticks(range(pd.DataFrame(X_train).shape[1]), names)
plt.show()# Make predictions
test_scores(Xgb)

"""**FINALIZE MODEL**

*   Create standalone model on entire training dataset
"""

def rmsle(y,y_):
    log1 = np.nan_to_num(np.array([np.log(v + 1) for v in y]))
    log2 = np.nan_to_num(np.array([np.log(v + 1) for v in y_]))
    calc = (log1 - log2) ** 2
    return np.sqrt(np.mean(calc))
def score(y, y_):
    print('r square  ', metrics.r2_score(y, y_))
    print('Adjusted r square:{}'.format(1 - (1-metrics.r2_score(y, y_))*(len(y)-1)/(len(y)-X_train.shape[1]-1)))
    print('MAPE:{}'.format(np.mean(np.abs((y - y_) / y))*100))
    print('MSE:', metrics.mean_squared_error(y, y_))
    print('RMSE:', np.sqrt(metrics.mean_squared_error(y, y_)))
    print('RMSLE:',rmsle(y_test,y_pred))
def scores(model):
    print('<<<------------------- Training Data Score --------------------->')
    print()
    #Predicting result on Training data
    y_pred = model.predict(X)
    score(y,y_pred)
    print('RMSLE:',rmsle(y,y_pred))

train.columns

test.columns

train.shape

test.shape

a=pd.read_csv('test.csv')

test_pickup_datetime=a['pickup_datetime']

# Instantiate a xgb regressor: xgb
Xgb = XGBRegressor(subsample= 0.1, reg_alpha= 0.08685113737513521, n_estimators= 200, max_depth= 3, learning_rate=0.05, colsample_bytree= 0.7000000000000001, colsample_bynode=0.7000000000000001, colsample_bylevel=0.9000000000000001)

# Fit the regressor to the data
Xgb.fit(X,y)

# Compute and print the coefficients
xgb_features = Xgb.feature_importances_
print(xgb_features)

# Sort feature importances in descending order
indices = np.argsort(xgb_features)[::1]

# Rearrange feature names so they match the sorted feature importances
names = [test.columns[i] for i in indices]

# Creating plot
fig = plt.figure(figsize=(20,10))
plt.title("Feature Importance")

# Add horizontal bars
plt.barh(range(pd.DataFrame(X_train).shape[1]),xgb_features[indices],align = 'center')
plt.yticks(range(pd.DataFrame(X_train).shape[1]), names)
plt.savefig(' xgb1 feature importance')
plt.show()
scores(Xgb)

# Predictions
pred = Xgb.predict(test.values)
pred_results_wrt_date = pd.DataFrame({"pickup_datetime":test_pickup_datetime,"fare_amount" : pred})
pred_results_wrt_date.to_csv("predictions_xgboost.csv",index=False)

pred_results_wrt_date

# Save the model as a pickle in a file 
joblib.dump(Xgb, 'cab_fare_xgboost_model.pkl') 
  
# # Load the model from the file 
# Xgb_from_joblib = joblib.load('cab_fare_xgboost_model.pkl